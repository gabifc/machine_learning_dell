{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_tree.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPavCJlWjCH5caeHHzT7sJu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabifc/machine_learning_dell/blob/main/decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSOjYGIPwW4m"
      },
      "source": [
        "## **Árvores de Decisão**\n",
        "###Conjunto de regras \"se-então\" utilizadas para regressão e classificação\n",
        "###Como obter a árvore a partir dos dados?\n",
        "Utiliza-se um índice de impureza do resultado de uma divisão. Para verificar a qualidade da divisão dos dados.\n",
        "Vários índices podem ser utilizados (gini, entropia...)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVU2wJzrwYoT"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "#importando os datasets boston e iris\n",
        "from sklearn.datasets import load_boston, load_iris\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9jHrVQoIomm"
      },
      "source": [
        "## Classificação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpfP2hDsHLgv"
      },
      "source": [
        "# instanciando a árvore de decisão\n",
        "dt = DecisionTreeClassifier()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgPsy-aWHVG0"
      },
      "source": [
        "# carregando os dados em x e y\n",
        "X,y = load_iris(return_X_y=True)\n",
        "# dividindo os dados de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABXhz8-TH2kK",
        "outputId": "ff1fa6c8-dde9-4fc4-8fe6-87cddfeaef46"
      },
      "source": [
        "#treinando a árvore\n",
        "dt.fit(X_train,y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "902dP-QsIH5Q",
        "outputId": "5a60c8ad-d80e-40cb-f27d-128d1b218ba9"
      },
      "source": [
        "dt.score(X_test,y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM-4Z_WoIRTj"
      },
      "source": [
        "## Regressão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntnd8kKOILUR"
      },
      "source": [
        "dtr = DecisionTreeRegressor()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPZP5bvrIWFy"
      },
      "source": [
        "X,y = load_boston(return_X_y=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmDOM9x_KRMl"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXYiDg5gKVsW",
        "outputId": "761986d9-eb24-4ec2-8d70-51a5033090a0"
      },
      "source": [
        "dtr.fit(X_train,y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JffNV3LKdr9",
        "outputId": "857b8921-ad00-4613-9422-d490de9e556e"
      },
      "source": [
        "dtr.score(X_test,y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7371640764207488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5OkblZSK8VJ"
      },
      "source": [
        "A árvore de decisão já possui um alto nível de overfitting. Ela tenta sempre carregar todo o conjunto de dados e tenta separá-los perfeitamente.  \n",
        "No que isso implica?  \n",
        "Uma vez que tentamos separar o conjunto de train perfeitamente temos o overfiting. Se existirem dados outliers, dados que não deveriam pertencer à normalidade nós teríamos uma divisão desses dados os tratando como se eles fossem, de fato, dados válidos e temos o caso de overfitting.  \n",
        "O overfitting se evidencia quando elas conseguem separar perfeitamente os dados. Se o objetivo da arvore é separar perfeitamente os dados, pq isso seria ruim?  \n",
        "O nosso conjunto de dados de train mostra algumas das caracteristicas dos nossos dados, logo iremos tentar extrair os padrões desse conjunto de dados para que nossa árvore de decisão consiga capturar esses padrões e classificar novos elementos, de acordo com os padrões encontrados.  \n",
        "Imagine: Você está querendo estudar para uma prova e se vc estuda a mesma questão vc vai aprender apenas uma coisa, não vai generalizar o conhecimento sobre o assunto.  \n",
        "Usando essa analogia, podemos trazer esse conhecimento para as árvores de decisão.  \n",
        "Se as aŕvores aprendem perfeitamente a separa um conjunto de dados a classificar um conjunto de dados, ela pode classificar tão perfeitamente que não conseguirão generalizar esse aprendizado para outros conjuntos. Logo, é melhor que possamos trabalhar com uma arvore que não clssifique tão pergfitamente esse conjunto de dados mas que consiga generalizar para os demais. Ao invés de aprender pergeitamente um conjunto de dados a ideia é atender os conceitos daquele conjunto de dados.  \n",
        "Então uso uma altura menor da arvore que garantirá que não irá continuar fazendo as divisões de dados perfeitamente mas até um certo ponto garantido que coceitos gerais sejam aprendidos e o restante dos conceitos não seja perfeitamente aprendidos uma vez que se faz necessário que aprendamos pergeitamente o conjunto de dados de entrada mas sim pegar uma ideia do que esses conjuntos de dados significam.  \n",
        "Temos que diminuir a quantidade de overfitting neste conjunto de dados aplicando alguma tecnica para podar a arvore, ou seja, evitar que ela tenha tantos nós.  \n",
        "Então neste momento vamos retreinar a arvore utilizando um limitador da quantidade de regras que serão feitas. Ao limitar essas quantidade de regras diminui a quantidade de nós puros. A divisão dos dados pode não ficar pura, por exermplo no caso da classe, irei utilizar a maioria das classes desse nó.  \n",
        "Ex: uma situação de um né que tem 5 salmão e um badejo e não iremos continuar separando esses conjuntos de dados, mas iremos escolher a classe como a classe dos salmoes. Qualquer elemento que chegue a esse nível será classificado como salmão, mesmo que o nó não esteja puro, irei escolher a maioria.  \n",
        "No caso de um conjunto de dados de regressão iremos escolher a média dos valores que estão ali.  \n",
        "Então, vou reaplicar o modelo executando poda na arvore.  \n",
        "O parâmetro max_depth indica a altura máxima da árvore de decisão. Essa altura determina a quantidade de divisões feitas sobre o dado de treino. Diminuindo o número de divisões, é possível evitar uma divisão perfeita dos dados de treino, evitando assim o overfit. Já n_neighbors é um parâmetro do modelo KNN, train_size é um parâmetro da função train_test_split e num_samples não é um parâmetro válido para árvores de decisão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQFDoP-DKeHH"
      },
      "source": [
        "# parametro de profundidade máxima onde coloco a altura max da arvore \n",
        "# e quanto menor ela for menos divisões teremos e se for pequena demais teremos underffiting e se for muito alta overffiting\n",
        "dt = DecisionTreeClassifier(max_depth=5)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXBsEPxeuK7C"
      },
      "source": [
        "X,y = load_iris(return_X_y=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAKMNpbbufxX"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nYvbs9yukXJ",
        "outputId": "47bd42df-e9b3-4d7d-d63b-f6f1996d94cd"
      },
      "source": [
        "dt.fit(X_train,y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKmDfdrIum9I",
        "outputId": "6f2c38d1-49cc-4696-a4f6-c26f938a8e35"
      },
      "source": [
        "# este score é diretamente afetado pela altura da arvore no parametro (max_depth=5).\n",
        "# com 3 baixou para 92%\n",
        "dt.score(X_test,y_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqt9Fuzvvmis"
      },
      "source": [
        "#**Selecionando Features com Árvores de Decisão**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxRe5jCxvTX9"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf7x3C0iv0D3"
      },
      "source": [
        "dt = DecisionTreeClassifier()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwLPXWuZwB7C"
      },
      "source": [
        "X,y = load_breast_cancer(return_X_y=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DuNPFJEwFIh",
        "outputId": "0ee4888e-fb35-4c3f-9f7f-f072db895ed1"
      },
      "source": [
        "dt.fit(X,y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOItxXdPwHe8",
        "outputId": "53d76dd7-1bf5-4e90-ea14-d3c60b176aaa"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCLuy5C43Dzh"
      },
      "source": [
        "O feature_importances_ é um atributo que determina a importância de cada coluna na classificação em um modelo de árvores de decisão. Já fit e score são, respectivamente, métodos para treinar e avaliar um modelo e max_depth é um parâmetro indicado para evitar que os dados sejam divididos perfeitamente ocasionando overfit do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk07I0jUwKCP",
        "outputId": "9afb27b1-0301-446c-8f46-4543a3bb3541"
      },
      "source": [
        "# para diminuir o conjunto de dados. Terei o resultado da importancia de cada um dos atributos\n",
        "dt.feature_importances_"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.04248468, 0.        , 0.00563858, 0.        ,\n",
              "       0.        , 0.00877112, 0.        , 0.        , 0.        ,\n",
              "       0.007316  , 0.        , 0.        , 0.00204521, 0.00100384,\n",
              "       0.00563858, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.69559352, 0.0460766 , 0.        , 0.0110859 , 0.01440488,\n",
              "       0.        , 0.00183582, 0.15121369, 0.00689159, 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBWZTOjO0P5_"
      },
      "source": [
        "As features precisam ser retiradas aos poucos pq podem estar ligadas e atrapalhar a performance do modelo.  \n",
        "Para eliminar atributos com segurança em uma árvore de decisão, deve-se eliminar o atributo com menor importância, treinar o modelo novamente para verificar como foi a evolução da avaliação do modelo e repetir este processo até que a avaliação do modelo estabilize ou piore. Um especialista pode ajudar na seleção, mas ele estará se utilizando apenas de seu conhecimento e não de uma técnica proveniente da estatística de importância dos atributos. Vale dizer que, eliminar vários atributos de uma vez pode implicar em perdas do modelo, uma vez que atributos de baixa importância podem se correlacionar com outros de alta importância. Por fim, o max_depth evita overfit, não servindo pra eliminação de features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZssY3qTwNTQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}